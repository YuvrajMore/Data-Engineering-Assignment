{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2 as ps2\n",
    "import yfinance as yf\n",
    "import sqlalchemy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import scipy.stats as stats\n",
    "import json\n",
    "import pyarrow.parquet as pq\n",
    "import re\n",
    "import talib as ta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "## imports for reading multiple files\n",
    "import glob\n",
    "import os\n",
    "import dask\n",
    "import dask_expr\n",
    "dask.config.set({'dataframe.query-planning':True})\n",
    "import dask.dataframe as ddf\n",
    "\n",
    "## setup logger to log flow of data creation\n",
    "import logging\n",
    "logger = logging.getLogger('data_ETLC_log')\n",
    "formatter = logging.Formatter('%(asctime)s | %(name)s | %(levelname)s | %(message)s')\n",
    "file_h = logging.FileHandler('data_ETL.log')\n",
    "file_h.setFormatter(formatter)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.addHandler(file_h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Connection and Cursor to PostgreSQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'psycopg2.extensions.connection'>\n",
      "<class 'psycopg2.extensions.cursor'>\n"
     ]
    }
   ],
   "source": [
    "## create connection with database fin_data and username fin_user\n",
    "try:\n",
    "    con = ps2.connect(\"host=127.0.0.1 dbname=fin_data user=fin_user password=fin_user\")\n",
    "    print(type(con))\n",
    "    logger.info(f\"Created connection of type {type(con)}\")\n",
    "except:\n",
    "    print(\"Could not connect to database\")\n",
    "    logger.error(\"Could not connect to database\", exc_info=True)\n",
    "\n",
    "## create cursor\n",
    "try:\n",
    "    cur = con.cursor()\n",
    "    print(type(cur))\n",
    "    logger.info(f\"Created cursor object of type {type(cur)}\")\n",
    "except:\n",
    "    print(\"Could not create cursor object\")\n",
    "    logger.error(\"Could not create cursor object\", exc_info=True)\n",
    "\n",
    "## set autocommit to true\n",
    "con.set_session(autocommit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Equity Table and insert values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['^NSEI','RELIANCE.NS', 'BHARTIARTL.NS', 'ASIANPAINT.NS','HDFCBANK.NS', 'ICICIBANK.NS', 'SBIN.NS','TCS.NS', 'MPHASIS.NS', 'TECHM.NS']\n",
    "try:\n",
    "    cur.execute(\"CREATE TABLE IF NOT EXISTS equity (equity_id serial PRIMARY KEY, company_name varchar, ticker varchar UNIQUE NOT NULL, sector varchar)\")\n",
    "    logger.info(\"Equity table created succesfully.\")\n",
    "except:\n",
    "    logger.error(\"Could not create table in database.\", exc_info=True)\n",
    "    raise Exception(\"Could not create table in database.\")\n",
    "for ticker_name in names:\n",
    "    try:\n",
    "        company_name = yf.Ticker(ticker_name).info['longName']\n",
    "        logger.info(f\"Equity retrieved for {company_name}.\")\n",
    "    except:\n",
    "        logger.error(f\"Could not retrieve data for {company_name}\", exc_info=True)\n",
    "        raise Exception(f\"Could not retrieve data for {company_name}\")\n",
    "    if ticker_name!='^NSEI':\n",
    "        sector = yf.Ticker(ticker_name).info['sector']\n",
    "    else:\n",
    "        sector = None\n",
    "    cur.execute(\"INSERT INTO equity (company_name, ticker, sector) values (%s, %s, %s)\",(company_name, ticker_name, sector))\n",
    "    logger.info(f\"Data for equity {company_name} added to equity table in database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions to get equity_id from ticker names or file path using equity table\n",
    "# Functions to find outlier and flag outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'NSEI': 1, 'RELIANCE': 2, 'BHARTIARTL': 3, 'ASIANPAINT': 4, 'HDFCBANK': 5, 'ICICIBANK': 6, 'SBIN': 7, 'TCS': 8, 'MPHASIS': 9, 'TECHM': 10}\n"
     ]
    }
   ],
   "source": [
    "## path to data source\n",
    "path = \"D:\\\\Study\\\\DE\\\\Project\\\\data_source\"\n",
    "\n",
    "## dictionary formed by reading equity table from database, to create the foreign key by replacing ticker name with equity_id\n",
    "equity_id_names = pd.read_sql(\"SELECT equity_id, ticker from equity\", con=con, index_col='ticker')\n",
    "equity_id_name_dict = equity_id_names.to_dict()['equity_id']\n",
    "equity_id_name_dict = {key.strip(\"^\").split('.')[0]: int(value) for (key,value) in equity_id_name_dict.items()}\n",
    "print(equity_id_name_dict)\n",
    "\n",
    "## this function takes dataframe at input, returns dataframe with file_path replaced by equity_id, because in source directory there is one file for each equity\n",
    "## extract filename from full path, file path column is created with name file_path, when calling dask.read_csv\n",
    "## file_path requires cleaning after reading from table,then use equity_id_name_dict to replace file_path with equity_id\n",
    "## map equity name to id and convert it to integer\n",
    "\n",
    "## data integrity check -> raises type error if arguement is not a dataframe or series from dask or pandas\n",
    "## data integrity check -> raise value error if dataframe does not contain a column named file_path\n",
    "\n",
    "def file_path_to_id(df):\n",
    "    if type(df) not in [ddf.DataFrame, pd.DataFrame, pd.Series, ddf.Series]:\n",
    "        logger.error(f\"Arguement of type {type(df)} was provided\", exc_info=True)\n",
    "        raise TypeError(\"Type of given arguement is not acceptable for this function\")\n",
    "    if 'file_path' not in df.columns:\n",
    "        logger.error(f\"No column named file_path found in given dataframe.\", f\"Columns present in given dataframe are:{df.columns}\", exc_info=True)\n",
    "        raise ValueError(\"Given dataframe does not contain a column named file_path.\")\n",
    "    df['file_path'] = df['file_path'].apply(lambda x: re.search(r'([^/]+)\\.(json|csv)$', x).group(1))\n",
    "    df = df.rename(columns={'file_path':'equity_id'})\n",
    "\n",
    "    df['equity_id'] = df['equity_id'].map(equity_id_name_dict)\n",
    "    df['equity_id'] = df['equity_id'].astype(int)\n",
    "    logger.info(\"Dataframe file_path column is modified succesfully.\")\n",
    "    return df.sort_values('equity_id')\n",
    "\n",
    "## this function takes and cleans ticker name instead of file path and maps equity_id to it.\n",
    "## data integrity check -> exactly similar to previous function\n",
    "\n",
    "def ticker_name_to_id(df):\n",
    "    if type(df) not in [ddf.DataFrame, pd.DataFrame, pd.Series, ddf.Series]:\n",
    "        logger.error(f\"Arguement of type {type(df)} was provided\", exc_info=True)\n",
    "        raise TypeError(\"Given arguement is not a Dataframe\")\n",
    "    if 'ticker' not in df.columns:\n",
    "        logger.error(f\"No column named ticker found in given dataframe.\", f\"Columns present in given dataframe are:{df.columns}\", exc_info=True)\n",
    "        raise ValueError(\"Given dataframe does not contain a column named ticker.\")\n",
    "    \n",
    "    df['ticker'] = df['ticker'].str.replace(\".NS\", \"\")\n",
    "    df['ticker'] = df['ticker'].map(equity_id_name_dict)\n",
    "    df.rename(columns={'ticker':'equity_id'}, inplace=True)\n",
    "    df['equity_id'] = df['equity_id'].astype(int)\n",
    "    logger.info(\"Dataframe ticker column is modified succesfully.\")\n",
    "    return df\n",
    "\n",
    "## function to find out outliers in a dataframe for each column\n",
    "## using z score method to find outliers\n",
    "def find_outlier(df, col):\n",
    "    if type(df) not in [ddf.DataFrame, pd.DataFrame, pd.Series, ddf.Series]:\n",
    "        logger.error(f\"Arguement of type {type(df)} was provided\", exc_info=True)\n",
    "        raise TypeError(\"Given arguement is not a Dataframe\")\n",
    "    if type(df) == ddf.DataFrame:\n",
    "        df = df.compute()\n",
    "    z = np.abs(stats.zscore(df[col]))\n",
    "    df[f'is_outlier_{col}'] = np.where(z>3, True, False)\n",
    "    return df\n",
    "\n",
    "## function to set final outlier column based on other outlier columns\n",
    "## if final column has value true, then that row should be removed as it contains an outlier\n",
    "def overall_outlier(df):\n",
    "    if type(df) not in [ddf.DataFrame, pd.DataFrame, pd.Series, ddf.Series]:\n",
    "        logger.error(f\"Arguement of type {type(df)} was provided\", exc_info=True)\n",
    "        raise TypeError(\"Given arguement is not a Dataframe\")\n",
    "    if type(df) == ddf.DataFrame:\n",
    "        df = df.compute()\n",
    "    df['outlier'] = np.where(((df['is_outlier_Open'] == True) |\\\n",
    "         (df['is_outlier_High'] == True) |\\\n",
    "             (df['is_outlier_Low'] == True) |\\\n",
    "                 (df['is_outlier_Close'] == True) |\\\n",
    "                    (df['is_outlier_Volume'] == True)\n",
    "    ), True, False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load split data and dividends data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records loaded from dividend data: 292\n",
      "         Date  dividend  equity_id\n",
      "0  1996-05-24    0.7430          2\n",
      "1  1997-05-16    0.8049          2\n",
      "2  1998-05-06    0.8668          2\n",
      "3  1999-05-07    0.9287          2\n",
      "4  2000-04-06    0.9906          2\n"
     ]
    }
   ],
   "source": [
    "## using pandas read_csv to read files\n",
    "## calling dataframe.round function to round of dividend values to 4 decimal digits\n",
    "## remove .NS from ticker name, replace ticker name by equity_id and rename columns\n",
    "\n",
    "div_path = r\"D:\\Study\\DE\\Project\\data_source\\splits_dividends_data\\dividends_data.csv\"\n",
    "div_data = pd.read_csv(div_path)\n",
    "\n",
    "## data integrity check -> check for duplicate/null records, if higher than threshold throw error, else drop and warn\n",
    "if div_data.duplicated().sum()>0:\n",
    "    if div_data.duplicated().sum()>10:\n",
    "        print(f\"High number of duplicate records found: {div_data.duplicated().sum()}\")\n",
    "        logger.error(f\"High number of duplicate records found: {div_data.duplicated().sum()}\", exc_info=True)\n",
    "        raise ValueError(\"Duplicate records present in dividends dataframe\")\n",
    "    else:\n",
    "        div_data = div_data.drop_duplicates(keep='first')\n",
    "        logger.info(f\"Dropped {div_data.duplicated().sum()} records from dividends dataframe due to duplication\")\n",
    "        warnings.warn(f\"Dropped {div_data.duplicated().sum()} records from dividends dataframe due to duplication\")\n",
    "\n",
    "if div_data.isna().sum().sum()>0:\n",
    "    if div_data.isna().sum().sum()>10:\n",
    "        print(f\"High number of null records found: {div_data.isna().sum().sum()}\")\n",
    "        logger.error(f\"High number of null records found: {div_data.isna().sum().sum()}\", exc_info=True)\n",
    "        raise ValueError(\"Null records present in dividends dataframe\")\n",
    "    else:\n",
    "        div_data = div_data.drop_na(keep='first')\n",
    "        logger.info(f\"Dropped {div_data.isna().sum()} records from dividends dataframe due to null values\")\n",
    "        warnings.warn(f\"Dropped {div_data.isna().sum()} records from dividends dataframe due to null values\")\n",
    "\n",
    "## data integrity check -> standardize date format to ensure constistency\n",
    "div_data['Date'] = pd.to_datetime(div_data['Date']).dt.date\n",
    "div_data['dividend'] = div_data['dividend'].round(4).astype(float)\n",
    "div_data = ticker_name_to_id(div_data)\n",
    "\n",
    "print(f\"Number of records loaded from dividend data: {div_data.shape[0]}\")\n",
    "logger.info(f\"Number of records loaded from dividend data: {div_data.shape[0]}\")\n",
    "div_data.to_parquet(path='D:\\\\Study\\\\DE\\\\Project\\\\data_destination\\\\dividend_data.parquet', compression='snappy')\n",
    "print(div_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records from splits data: 18\n",
      "         Date split_ratio  equity_id\n",
      "0  1997-10-27         2:1          2\n",
      "1  2009-11-26         2:1          2\n",
      "2  2017-09-07         2:1          2\n",
      "3  2009-07-24         2:1          3\n",
      "4  2003-08-22         3:2          4\n"
     ]
    }
   ],
   "source": [
    "## using pandas read_csv to read files\n",
    "## calling dataframe.round function to round of dividend values to 4 decimal digits\n",
    "## remove .NS from ticker name, replace ticker name by equity_id and rename columns\n",
    "\n",
    "split_path = r\"D:\\Study\\DE\\Project\\data_source\\splits_dividends_data\\splits_data.csv\"\n",
    "split_data = pd.read_csv(split_path)\n",
    "split_data.rename(columns={\"splitRatio\":\"split_ratio\"}, inplace=True)\n",
    "\n",
    "## data integrity check -> check for duplicate/null records, if higher than threshold throw error, else drop and warn\n",
    "if split_data.duplicated().sum()>0:\n",
    "    if split_data.duplicated().sum()>10:\n",
    "        print(f\"High number of duplicate records found: {split_data.duplicated().sum()}\")\n",
    "        logger.error(f\"High number of duplicate records found: {split_data.duplicated().sum()}\", exc_info=True)\n",
    "        raise ValueError(\"Duplicate records present in splits dataframe\")\n",
    "    else:\n",
    "        split_data = split_data.drop_duplicates(keep='first')\n",
    "        logger.info(f\"Dropped {split_data.duplicated().sum()} records from splits dataframe due to duplication\")\n",
    "        warnings.warn(f\"Dropped {split_data.duplicated().sum()} records from splits dataframe due to duplication\")\n",
    "if split_data.isna().sum().sum()>0:\n",
    "    if split_data.isna().sum().sum()>10:\n",
    "        print(f\"High number of null records found: {split_data.isna().sum().sum()}\")\n",
    "        logger.error(f\"High number of null records found: {split_data.isna().sum().sum()}\", exc_info=True)\n",
    "        raise ValueError(\"Null records present in splits dataframe\")\n",
    "    else:\n",
    "        split_data = split_data.dropna(keep='first')\n",
    "        logger.info(f\"Dropped {split_data.isna().sum()} records from splits dataframe due to null values\")\n",
    "        warnings.warn(f\"Dropped {split_data.isna().sum()} records from splits dataframe due to null values\")\n",
    "\n",
    "## data integrity check -> search for regex pattern in split_ratio to ensure split is present in correct format else raises valueError\n",
    "split_string_quality = split_data['split_ratio'].str.find(r\"\\d:\\d\").astype(bool)\n",
    "if False in split_string_quality:\n",
    "    print([val.index for val in split_string_quality if val==False])\n",
    "    logger.error(\"Split ratio data not in correct format. Correct format is a string like -> number:number\", exc_info=True)\n",
    "    raise ValueError(\"Split ratio data not in correct format. Correct format is a string like -> number:number\")\n",
    "\n",
    "## data integrity check -> standardize date format to ensure constistency\n",
    "split_data['Date'] = pd.to_datetime(split_data['Date']).dt.date\n",
    "\n",
    "split_data = ticker_name_to_id(split_data)\n",
    "\n",
    "print(f\"Number of records from splits data: {split_data.shape[0]}\")\n",
    "logger.info(f\"Number of records loaded from splits data: {split_data.shape[0]}\")\n",
    "div_data.to_parquet(path='D:\\\\Study\\\\DE\\\\Project\\\\data_destination\\\\split_data.parquet', compression='snappy')\n",
    "print(split_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create tables in database for split and dividend data, insert values from dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create tables and iterate through rows of dataframe to insert values.\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS split_data (split_id serial PRIMARY KEY, \n",
    "date date NOT NULL, \n",
    "split_ratio varchar,\n",
    "equity_id int,\n",
    "FOREIGN KEY(equity_id) REFERENCES equity(equity_id))\"\"\")\n",
    "logger.info(\"Succesfully created table names split_data\")\n",
    "\n",
    "for index, row in split_data.iterrows():\n",
    "    cur.execute(\"INSERT INTO split_data (Date, split_ratio, equity_id) values(%s, %s, %s)\", (row['Date'], row['split_ratio'], row['equity_id']))\n",
    "logger.info(f\"Succesfully inserted {split_data.shape[0]} rows to split_data table in database\")\n",
    "\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS dividend_data (dividend_id SERIAL PRIMARY KEY,\n",
    "date date NOT NULL,\n",
    "dividend numeric,\n",
    "equity_id int,\n",
    "FOREIGN KEY(equity_id) REFERENCES equity(equity_id))\"\"\")\n",
    "logger.info(\"Succesfully created table names dividend_data\")\n",
    "\n",
    "for index, row in div_data.iterrows():\n",
    "    cur.execute(\"INSERT INTO dividend_data (date, dividend, equity_id) values(%s, %s, %s)\", (row['Date'], row['dividend'], row['equity_id']))\n",
    "logger.info(f\"Succesfully inserted {div_data.shape[0]} rows to dividend_data table in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data from source, merge and insert in database (execute once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers found: 162\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>equity_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>10479.95</td>\n",
       "      <td>10525.50</td>\n",
       "      <td>10447.15</td>\n",
       "      <td>10458.35</td>\n",
       "      <td>176000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>2022-03-07</td>\n",
       "      <td>15867.95</td>\n",
       "      <td>15944.60</td>\n",
       "      <td>15711.45</td>\n",
       "      <td>15863.15</td>\n",
       "      <td>585400</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>16339.45</td>\n",
       "      <td>16456.00</td>\n",
       "      <td>16133.80</td>\n",
       "      <td>16245.35</td>\n",
       "      <td>456100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>16723.20</td>\n",
       "      <td>16768.95</td>\n",
       "      <td>16442.95</td>\n",
       "      <td>16498.05</td>\n",
       "      <td>442100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>16593.10</td>\n",
       "      <td>16678.50</td>\n",
       "      <td>16478.65</td>\n",
       "      <td>16605.95</td>\n",
       "      <td>517700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date      Open      High       Low     Close  Volume  equity_id\n",
       "0    2018-03-01  10479.95  10525.50  10447.15  10458.35  176000          1\n",
       "987  2022-03-07  15867.95  15944.60  15711.45  15863.15  585400          1\n",
       "986  2022-03-04  16339.45  16456.00  16133.80  16245.35  456100          1\n",
       "985  2022-03-03  16723.20  16768.95  16442.95  16498.05  442100          1\n",
       "984  2022-03-02  16593.10  16678.50  16478.65  16605.95  517700          1"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Using dask library for reading all csvs in a directory. include_path_column is used to get file name\n",
    "## then used file_path_to_id for transformation\n",
    "## data integrity check -> standardize date format to ensure constistency\n",
    "## data integrity check -> check for duplicate and null records, if higher than threshold throw error, else drop and warn\n",
    "\n",
    "df_csv = ddf.read_csv(f\"D:\\\\Study\\\\DE\\\\Project\\\\data_source\\\\*.csv\", include_path_column=\"file_path\")\n",
    "before_length = df_csv.compute().shape[0]\n",
    "after_length = df_csv.compute().drop_duplicates().shape[0]\n",
    "num_duplicates = before_length - after_length\n",
    "num_na = df_csv.compute().isna().sum().sum()\n",
    "\n",
    "if num_duplicates>0:\n",
    "    if num_duplicates>10:\n",
    "        print(f\"High number of duplicate records found: {num_duplicates}\")\n",
    "        logger.error(f\"High number of duplicate records found: {num_duplicates}\", exc_info=True)\n",
    "        raise ValueError(\"Duplicate records present in csv dataframe\")\n",
    "    else:\n",
    "        df_csv = df_csv.drop_duplicates(keep='first')\n",
    "        logger.info(f\"Dropped {num_duplicates} records from csv dataframe due to duplication\")\n",
    "        warnings.warn(f\"Dropped {num_duplicates} records from csv dataframe due to duplication\")\n",
    "if num_na>0:\n",
    "    if num_na>10:\n",
    "        print(f\"High number of null records found: {num_na}\")\n",
    "        logger.error(f\"High number of null records found: {num_na}\", exc_info=True)\n",
    "        raise ValueError(\"Null records present in csv dataframe\")\n",
    "    else:\n",
    "        df_csv = df_csv.drop_na()\n",
    "        logger.info(f\"Dropped {num_na} records from csv dataframe due to null values\")\n",
    "        warnings.warn(f\"Dropped {num_na} records from csv dataframe due to null values\")\n",
    "\n",
    "df_csv['Date'] = ddf.to_datetime(df_csv['Date']).dt.date\n",
    "df_csv = df_csv.round(2)\n",
    "df_csv = file_path_to_id(df_csv)\n",
    "\n",
    "## convert to pandas dataframe before calculating outliers\n",
    "for col in df_csv.compute().columns:\n",
    "    if col in ['Date','equity_id']:\n",
    "        continue\n",
    "    df_csv = find_outlier(df_csv,col)\n",
    "\n",
    "df_csv = overall_outlier(df_csv)\n",
    "print(f\"Outliers found: {df_csv['outlier'].sum()}\")\n",
    "logger.info(f\"Outliers found: {df_csv['outlier'].sum()}\")\n",
    "\n",
    "## if too many outliers are present, then raise exception\n",
    "if(df_csv['outlier'].sum() > (df_csv.shape[0]/100)*2.5):\n",
    "    logger.error(\"Number of outliers is more than 2.5 percent\", exc_info=True)\n",
    "    raise Exception(\"Too many outliers found.\")\n",
    "\n",
    "## drop columns that are not needed as we have final outlier column\n",
    "df_csv.drop(columns=['is_outlier_Open', 'is_outlier_High', 'is_outlier_Low', 'is_outlier_Close', 'is_outlier_Volume'], inplace=True)\n",
    "## filter outliers based on column\n",
    "df_csv = df_csv[df_csv['outlier']==False]\n",
    "## drop column as outliers are removed\n",
    "df_csv.drop(columns=['outlier'], inplace=True)\n",
    "\n",
    "logger.info(\"Successfully created single combined dataframe of all csv files, outliers are removed\")\n",
    "df_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outliers found: 176\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>equity_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>303.57</td>\n",
       "      <td>304.88</td>\n",
       "      <td>295.42</td>\n",
       "      <td>296.10</td>\n",
       "      <td>18986889</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>2022-03-04</td>\n",
       "      <td>681.27</td>\n",
       "      <td>696.60</td>\n",
       "      <td>675.35</td>\n",
       "      <td>678.36</td>\n",
       "      <td>18697223</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>2022-03-03</td>\n",
       "      <td>707.50</td>\n",
       "      <td>711.78</td>\n",
       "      <td>683.69</td>\n",
       "      <td>688.47</td>\n",
       "      <td>23190595</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>2022-03-02</td>\n",
       "      <td>706.90</td>\n",
       "      <td>714.74</td>\n",
       "      <td>697.78</td>\n",
       "      <td>704.78</td>\n",
       "      <td>25252492</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>2022-02-28</td>\n",
       "      <td>712.77</td>\n",
       "      <td>734.26</td>\n",
       "      <td>705.33</td>\n",
       "      <td>732.24</td>\n",
       "      <td>20547432</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Date    Open    High     Low   Close    Volume  equity_id\n",
       "0    2018-03-01  303.57  304.88  295.42  296.10  18986889          6\n",
       "989  2022-03-04  681.27  696.60  675.35  678.36  18697223          6\n",
       "988  2022-03-03  707.50  711.78  683.69  688.47  23190595          6\n",
       "987  2022-03-02  706.90  714.74  697.78  704.78  25252492          6\n",
       "986  2022-02-28  712.77  734.26  705.33  732.24  20547432          6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using dask to read multiple json files, then using file_path_to_id function\n",
    "## data integrity check -> standardize date format to ensure constistency\n",
    "## data integrity check -> check for duplicate and null records, if higher than threshold throw error, else drop and warn\n",
    "\n",
    "df_json = ddf.read_json(f\"D:\\\\Study\\\\DE\\\\Project\\\\data_source\\\\*.json\", include_path_column=\"file_path\", orient='record')\n",
    "before_length = df_json.compute().shape[0]\n",
    "after_length = df_json.compute().drop_duplicates().shape[0]\n",
    "num_duplicates = before_length - after_length\n",
    "num_na = df_json.compute().isna().sum().sum()\n",
    "\n",
    "if num_duplicates>0:\n",
    "    if num_duplicates>10:\n",
    "        print(f\"High number of duplicate records found: {num_duplicates}\")\n",
    "        logger.error(f\"High number of duplicate records found: {num_duplicates}\", exc_info=True)\n",
    "        raise ValueError(\"Duplicate records present in json dataframe\")\n",
    "    else:\n",
    "        df_json = df_json.drop_duplicates(keep='first')\n",
    "        logger.info(f\"Dropped {num_duplicates} records from json dataframe due to duplication\")\n",
    "        warnings.warn(f\"Dropped {num_duplicates} records from json dataframe due to duplication\")\n",
    "if num_na>0:\n",
    "    if num_na>10:\n",
    "        print(f\"High number of null records found: {num_na}\")\n",
    "        logger.error(f\"High number of null records found: {num_na}\", exc_info=True)\n",
    "        raise ValueError(\"Null records present in json dataframe\")\n",
    "    else:\n",
    "        df_json = df_json.drop_na()\n",
    "        logger.info(f\"Dropped {num_na} records from json dataframe due to null values\")\n",
    "        warnings.warn(f\"Dropped {num_na} records from json dataframe due to null values\")\n",
    "\n",
    "df_json['Date'] = ddf.to_datetime(df_json['Date']).dt.date\n",
    "df_json = file_path_to_id(df_json)\n",
    "df_json = df_json.round(2)\n",
    "\n",
    "for col in df_json.compute().columns:\n",
    "    if col in ['Date','equity_id']:\n",
    "        continue\n",
    "    df_json = find_outlier(df_json,col)\n",
    "\n",
    "## similar approach to df_csv for removing outliers from dataframe\n",
    "df_json = overall_outlier(df_json)\n",
    "print(f\"Outliers found: {df_json['outlier'].sum()}\")\n",
    "logger.info(f\"Outliers found: {df_json['outlier'].sum()}\")\n",
    "\n",
    "if(df_json['outlier'].sum() > (df_json.shape[0]/100)*2.5):\n",
    "    logger.error(\"Number of outliers is more than 2.5 percent\", exc_info=True)\n",
    "    raise Exception(\"Too many outliers found.\")\n",
    "\n",
    "\n",
    "df_json.drop(columns=['is_outlier_Open', 'is_outlier_High', 'is_outlier_Low', 'is_outlier_Close', 'is_outlier_Volume'], inplace=True)\n",
    "df_json = df_json[df_json['outlier']==False]\n",
    "df_json.drop(columns=['outlier'], inplace=True)\n",
    "\n",
    "\n",
    "logger.info(\"Successfully created single combined dataframe of all json files\")\n",
    "df_json.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>equity_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>10479.95</td>\n",
       "      <td>10525.50</td>\n",
       "      <td>10447.15</td>\n",
       "      <td>10458.35</td>\n",
       "      <td>176000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>376.22</td>\n",
       "      <td>382.84</td>\n",
       "      <td>373.33</td>\n",
       "      <td>375.08</td>\n",
       "      <td>5837005</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>1073.45</td>\n",
       "      <td>1077.48</td>\n",
       "      <td>1061.95</td>\n",
       "      <td>1072.40</td>\n",
       "      <td>736217</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>900.94</td>\n",
       "      <td>907.65</td>\n",
       "      <td>895.52</td>\n",
       "      <td>898.57</td>\n",
       "      <td>1954920</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>845.28</td>\n",
       "      <td>854.31</td>\n",
       "      <td>841.90</td>\n",
       "      <td>844.08</td>\n",
       "      <td>4478295</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close   Volume  equity_id\n",
       "0  2018-03-01  10479.95  10525.50  10447.15  10458.35   176000          1\n",
       "0  2018-03-01    376.22    382.84    373.33    375.08  5837005          3\n",
       "0  2018-03-01   1073.45   1077.48   1061.95   1072.40   736217          4\n",
       "0  2018-03-01    900.94    907.65    895.52    898.57  1954920          5\n",
       "0  2018-03-01    845.28    854.31    841.90    844.08  4478295          2"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## using dask.concat() to concatenate dataframes formed from csv and json files.\n",
    "\n",
    "df = ddf.concat([df_csv, df_json])\n",
    "logger.info(\"Succesfully created concatenated dataframe of all csv and json files\")\n",
    "df.to_parquet(path='D:\\\\Study\\\\DE\\\\Project\\\\data_destination', compression='snappy', partition_on=['equity_id'])\n",
    "logger.info(\"OHLC Feed successfully saved to data_destination directory in parquet format.\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create table, user iterrows() to iterate through rows and insert data\n",
    "cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS OHLC_Feed (entry_id SERIAL PRIMARY KEY,\n",
    "    equity_id int,\n",
    "    date date,\n",
    "    open numeric,\n",
    "    high numeric,\n",
    "    low numeric,\n",
    "    close numeric,\n",
    "    volume int,\n",
    "    FOREIGN KEY(equity_id) REFERENCES equity(equity_id) )\"\"\")\n",
    "logger.info(\"Successfully created table named OHLC_Feed in database\")\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    cur.execute(\"INSERT INTO ohlc_feed (equity_id, date, open, high, low, close, volume) values (%s, %s, %s, %s, %s, %s, %s)\",\\\n",
    "    (row['equity_id'], row['Date'], row['Open'], row['High'], row['Low'], row['Close'], row['Volume']))\n",
    "logger.info(f\"Successfully inserted {df.shape[0]} rows to table named OHLC_Feed in database\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read daily updated entries from source, write new data to table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute once daily, preferably after market is closed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from file NSEI.csv is saved to table for ticker name NSEI\n",
      "[datetime.datetime(2024, 2, 22, 0, 0), 22020.3, 22252.5, 21875.25, 22217.45, 0]\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Data from file RELIANCE.csv is saved to table for ticker name RELIANCE\n",
      "[datetime.datetime(2024, 2, 22, 0, 0), 2936.3, 2969.9, 2916.0, 2963.5, 9246346]\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Data from file BHARTIARTL.csv is saved to table for ticker name BHARTIARTL\n",
      "[datetime.datetime(2024, 2, 22, 0, 0), 1137.6, 1138.75, 1097.65, 1135.55, 8642337]\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Data from file ASIANPAINT.csv is saved to table for ticker name ASIANPAINT\n",
      "[datetime.datetime(2024, 2, 22, 0, 0), 2983.0, 3027.2, 2932.1, 3017.4, 2615297]\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Data from file HDFCBANK.csv is saved to table for ticker name HDFCBANK\n",
      "[datetime.datetime(2024, 2, 22, 0, 0), 1417.75, 1428.8, 1412.2, 1419.55, 25277513]\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Data from file ICICIBANK.json is saved to table for ticker name ICICIBANK\n",
      "{'Date': datetime.datetime(2024, 2, 22, 0, 0), 'Open': 1056.0, 'High': 1066.0, 'Low': 1040.3, 'Close': 1062.7, 'Volume': 13430613}\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Data from file SBIN.json is saved to table for ticker name SBIN\n",
      "{'Date': datetime.datetime(2024, 2, 22, 0, 0), 'Open': 773.0, 'High': 773.0, 'Low': 757.0, 'Close': 765.9, 'Volume': 18796764}\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Data from file TCS.json is saved to table for ticker name TCS\n",
      "{'Date': datetime.datetime(2024, 2, 22, 0, 0), 'Open': 3971.0, 'High': 4094.95, 'Low': 3971.0, 'Close': 4087.1, 'Volume': 2981618}\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Data from file MPHASIS.json is saved to table for ticker name MPHASIS\n",
      "{'Date': datetime.datetime(2024, 2, 22, 0, 0), 'Open': 2680.9, 'High': 2723.95, 'Low': 2655.35, 'Close': 2698.45, 'Volume': 765168}\n",
      "####################################################################################################\n",
      "\n",
      "\n",
      "Data from file TECHM.json is saved to table for ticker name TECHM\n",
      "{'Date': datetime.datetime(2024, 2, 22, 0, 0), 'Open': 1307.0, 'High': 1330.5, 'Low': 1299.35, 'Close': 1328.1, 'Volume': 2201247}\n",
      "####################################################################################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## update db entries daily, add one last row from csv and json files\n",
    "## using os.listdir() to get list of files inside a directory, ignore if not a file\n",
    "## iterate through files, if ends with .csv then open and read lines\n",
    "## for csv files, last row is read as string, use split to convert to list and extract datetime object from string\n",
    "## for json files, last row is read as a dictionary, converting datetime object to date only\n",
    "## get ticker_name by splitting file name, use it as key to dictionary\n",
    "## use list indexing or dictionary keys and filename along with equity_id_name_dict to insert data\n",
    "\n",
    "## data integrity check -> when files are opened in read mode, date is in string format, convert using strptime()\n",
    "## data integrity check -> round off floats to last 2 digits\n",
    "## data integrity check -> convert volume to integer\n",
    "\n",
    "file_list = os.listdir(path)\n",
    "file_list = [file for file in file_list if os.path.isfile(path+'\\\\'+file)]\n",
    "file_list = sorted(file_list, key = lambda x: equity_id_name_dict[x.split('.')[0]])\n",
    "for file_name in file_list:\n",
    "    if file_name.endswith('.csv'):\n",
    "        last_row = \"\"\n",
    "        try:\n",
    "            with open(path+\"\\\\\"+file_name, 'r') as file:\n",
    "                last_row = (file.readlines()[-1])\n",
    "                logger.info(f\"Successfully read last row from file: {file_name}\")\n",
    "        except:\n",
    "            logger.error(f\"Could not read data from file:{file_name}\", exc_info=True)\n",
    "        last_row = last_row.strip('\\n').split(',')\n",
    "        last_row[0] = dt.datetime.strptime(last_row[0], \"%Y-%m-%d\")\n",
    "            \n",
    "        ticker_name = file_name.split('.')[0]\n",
    "\n",
    "        last_row[1] = round(float(last_row[1]),2)\n",
    "        last_row[2] = round(float(last_row[2]),2)\n",
    "        last_row[3] = round(float(last_row[3]),2)\n",
    "        last_row[4] = round(float(last_row[4]),2)\n",
    "        last_row[5] = int(last_row[5])\n",
    "\n",
    "        cur.execute(\"INSERT INTO ohlc_feed (equity_id, date, open, high, low, close, volume) values (%s, %s, %s, %s, %s, %s, %s)\",\\\n",
    "        (equity_id_name_dict[ticker_name],last_row[0],last_row[1],last_row[2],last_row[3],last_row[4],last_row[5]))\n",
    "        logger.info(f\"Successfully inserted last row from file: {file_name} to ohlc_feed table in database\")\n",
    "\n",
    "        print(f\"Data from file {file_name} is saved to table for ticker name {ticker_name}\")\n",
    "        print(last_row)\n",
    "        print(r\"#\" * 100)\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "    elif file_name.endswith('.json'):\n",
    "        last_row = {}\n",
    "        try:\n",
    "            with open(path+\"\\\\\"+file_name, 'r') as file:\n",
    "                last_row=json.load(file)[-1]\n",
    "                logger.info(f\"Successfully read last row from file: {file_name}\")\n",
    "        except:\n",
    "            logger.error(f\"Could not read data from file:{file_name}\", exc_info=True)\n",
    "        last_row['Date'] = dt.datetime.strptime(last_row['Date'], \"%Y-%m-%d\")\n",
    "\n",
    "        ticker_name = file_name.split('.')[0]\n",
    "\n",
    "        last_row['Open'] = round(float(last_row['Open']),2)\n",
    "        last_row['High'] = round(float(last_row['High']),2)\n",
    "        last_row['Low'] = round(float(last_row['Low']),2)\n",
    "        last_row['Close'] = round(float(last_row['Close']),2)\n",
    "        last_row['Volume'] = int(last_row['Volume'])\n",
    "\n",
    "        cur.execute(\"INSERT INTO ohlc_feed (equity_id, date, open, high, low, close, volume) values (%s, %s, %s, %s, %s, %s, %s)\",\\\n",
    "        (equity_id_name_dict[ticker_name],last_row['Date'],last_row['Open'],last_row['High'],last_row['Low'],last_row['Close'],last_row['Volume']))\n",
    "        logger.info(f\"Successfully inserted last row from file: {file_name} to ohlc_feed table in database\")\n",
    "\n",
    "        print(f\"Data from file {file_name} is saved to table for ticker name {ticker_name}\")\n",
    "        print(last_row)\n",
    "        print(r\"#\" * 100)\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating indicators for analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records read from database: 674\n",
      "Calculations are performed on date range printed below.\n",
      "[datetime.date(2023, 11, 6)]\n",
      "[datetime.date(2024, 2, 22)]\n"
     ]
    }
   ],
   "source": [
    "## taking more amount of data as some calculations are performed on windows of date, hence more past data is required\n",
    "start_date = (dt.date.today() - dt.timedelta(days=110)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "try:\n",
    "    df_ind = pd.read_sql(f\"select * from ohlc_feed where date > '{start_date}'\", con=con, index_col=\"entry_id\")\n",
    "    logger.info(f\"Successfully read {df_ind.shape[0]} rows from ohlc_feed table in database\")\n",
    "    print(f\"Number of records read from database: {df_ind.shape[0]}\")\n",
    "except:\n",
    "    logger.error(\"Could not query database\", exc_info=True)\n",
    "    raise Exception(\"Could not query database\")\n",
    "\n",
    "df_ind = df_ind.sort_values([\"equity_id\", \"date\"]).reset_index().drop(columns=['entry_id'])\n",
    "df_ind['date'] = pd.to_datetime(df_ind['date']).dt.date\n",
    "\n",
    "print(\"Calculations are performed on date range printed below.\")\n",
    "logger.info(f\"Range of calculation is from {df_ind.groupby('equity_id')['date'].min().unique()} to {df_ind.groupby('equity_id')['date'].max().unique()}.\")\n",
    "print(df_ind.groupby('equity_id')['date'].min().unique())\n",
    "print(df_ind.groupby('equity_id')['date'].max().unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For equity id: 1, Calculated 4 number of rows\n",
      "For equity id: 2, Calculated 60 number of rows\n",
      "For equity id: 3, Calculated 60 number of rows\n",
      "For equity id: 4, Calculated 60 number of rows\n",
      "For equity id: 5, Calculated 50 number of rows\n",
      "For equity id: 6, Calculated 60 number of rows\n",
      "For equity id: 7, Calculated 59 number of rows\n",
      "For equity id: 8, Calculated 51 number of rows\n",
      "For equity id: 9, Calculated 60 number of rows\n",
      "For equity id: 10, Calculated 60 number of rows\n"
     ]
    }
   ],
   "source": [
    "df_calc = pd.DataFrame()\n",
    "group = df_ind.groupby('equity_id')\n",
    "for equity_id, df_eq in group:\n",
    "    ## Simple moving average and standard deviation using pandas rolling function\n",
    "    ## Exponential moving average using pandas ewm (equity weighted) function\n",
    "    \n",
    "    ## not using moving_avg_close as bb_mid column calculated by bollinger band function has the exact same value\n",
    "    #df_eq['moving_avg_close'] = df_eq['close'].rolling(window=14).mean()\n",
    "    df_eq['exp_moving_avg_close'] = df_eq['close'].ewm(span=14, ignore_na=True).mean()\n",
    "    df_eq['moving_std_dev_close'] = df_eq['close'].rolling(window=14).std()\n",
    "    \n",
    "    ## momentum indicators\n",
    "    df_eq['rsi_close'] = ta.RSI(df_eq['close'], timeperiod=14)\n",
    "    df_eq['stochastic_osc_kline'],df_eq['stochastic_osc_dline'] = ta.STOCH(high=df_eq['high'], low=df_eq['low'], close=df_eq['close'], fastk_period=5, slowk_period=5, slowd_period=5)\n",
    "\n",
    "    ## volatitlity indicators, ATR and bollinger bands\n",
    "    df_eq['avg_true_range'] = ta.ATR(high=df_eq['high'], low=df_eq['low'], close=df_eq['close'], timeperiod=14)\n",
    "    df_eq['bb_up'], df_eq['bb_mid'], df_eq['bb_low'] = ta.BBANDS(df_eq['close'], timeperiod=14)\n",
    "    \n",
    "    ## volume indicators\n",
    "    df_eq['on_balance_volume'] = ta.OBV(df_eq['close'], df_eq['volume'])\n",
    "    df_eq['money_flow_index'] = ta.MFI(high=df_eq['high'], low=df_eq['low'], close=df_eq['close'], volume=df_eq['volume'], timeperiod=14)\n",
    "\n",
    "    ## drop columns which are already present in ohlc_feed\n",
    "    df_eq.drop(columns=['open','high','low','volume'], inplace=True)\n",
    "    df_eq = df_eq.round(2)\n",
    "    df_eq = df_eq.dropna()\n",
    "\n",
    "    ## daily log returns indicator\n",
    "    df_eq['daily_log_return'] = np.log(df_eq['close']/df_eq['close'].shift(1))\n",
    "    df_eq.drop(columns=['close'], inplace=True)\n",
    "    df_eq = df_eq.dropna()\n",
    "    \n",
    "\n",
    "    print(f\"For equity id: {equity_id}, Calculated {df_eq.shape[0]} number of rows\")\n",
    "    #print(df_eq.head())\n",
    "\n",
    "    ## concatenate dataframes, to form final table for inserting in database\n",
    "    df_calc=pd.concat([df_calc,df_eq])\n",
    "    logger.info(f\"Succesfully performed calculations on dataframe for equity_id: {equity_id}, Calculated {df_eq.shape[0]} number of rows and {df_eq.shape[1]} number of columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number NaN values in calculated dataframe: 0\n",
      "Number of rows in calculated dataframe: 524\n",
      "Number of columns in calculated dataframe: 14\n",
      "####################################################################################################\n",
      "Calculations are available for date range printed below.\n",
      "Starting from : ['2023-11-29']\n",
      "Starting from : ['2024-02-22']\n",
      "####################################################################################################\n"
     ]
    }
   ],
   "source": [
    "## check columns and number of rows in dataframe\n",
    "print(f\"Number NaN values in calculated dataframe: {df_calc.isna().sum().sum()}\")\n",
    "print(f\"Number of rows in calculated dataframe: {df_calc.shape[0]}\")\n",
    "print(f\"Number of columns in calculated dataframe: {df_calc.shape[1]}\")\n",
    "\n",
    "logger.info(f\"Number NaN values in calculated dataframe: {df_calc.isna().sum().sum()}\")\n",
    "logger.info(f\"Number of rows in calculated dataframe: {df_calc.shape[0]}\")\n",
    "logger.info(f\"Number of columns in calculated dataframe: {df_calc.shape[1]}\")\n",
    "\n",
    "print(r\"#\" * 100)\n",
    "print(\"Calculations are available for date range printed below.\")\n",
    "print(f\"Starting from : {df_calc.groupby('equity_id')['date'].min().unique().astype(str)}\")\n",
    "print(f\"Starting from : {df_calc.groupby('equity_id')['date'].max().unique().astype(str)}\")\n",
    "print(r\"#\" * 100)\n",
    "\n",
    "logger.info(\"Calculations are available for date range printed below.\")\n",
    "logger.info(f\"Starting from : {df_calc.groupby('equity_id')['date'].min().unique().astype(str)}\")\n",
    "logger.info(f\"Starting from : {df_calc.groupby('equity_id')['date'].max().unique().astype(str)}\")\n",
    "\n",
    "df_calc.to_parquet(path=\"D:\\\\Study\\\\DE\\\\Project\\\\data_destination\\\\short_term_analysis.parquet\", compression='snappy')\n",
    "\n",
    "#print(df_calc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Insert calculated data into a new table in database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    cur.execute(\"\"\"CREATE TABLE IF NOT EXISTS short_term_analysis (entry_id SERIAL PRIMARY KEY, equity_id int, date date, exp_moving_avg_close numeric, moving_std_dev_close numeric,\n",
    "    rsi_close numeric, stochastic_osc_kline numeric, stochastic_osc_dline numeric, avg_true_range numeric, bb_up numeric, bb_mid numeric, bb_low numeric,\n",
    "    on_balance_volume numeric, money_flow_index numeric, daily_log_return numeric, FOREIGN KEY(equity_id) REFERENCES equity(equity_id))\"\"\")\n",
    "    logger.info(\"Successfully created table named short_term_analysis\")\n",
    "except:\n",
    "    logger.error(\"Could not create table short_term_analysis in database\", exc_info=True)\n",
    "    raise Exception(\"Could not create table short_term_analysis\")\n",
    "\n",
    "for index, row in df_calc.iterrows():\n",
    "    try:\n",
    "        cur.execute(\"\"\"INSERT INTO short_term_analysis (equity_id, date, exp_moving_avg_close, moving_std_dev_close,\n",
    "        rsi_close, stochastic_osc_kline, stochastic_osc_dline, avg_true_range, \n",
    "        bb_up, bb_mid, bb_low, on_balance_volume, money_flow_index, daily_log_return)\n",
    "        values (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)\"\"\",\\\n",
    "        (row['equity_id'], row['date'], row['exp_moving_avg_close'], row['moving_std_dev_close'],\\\n",
    "        row['rsi_close'], row['stochastic_osc_kline'], row['stochastic_osc_dline'], row['avg_true_range'],\\\n",
    "        row['bb_up'], row['bb_mid'],row['bb_low'], row['on_balance_volume'], row['money_flow_index'], row['daily_log_return']))\n",
    "    except:\n",
    "        logger.error(\"Could not insert data into table short_term_analysis\", exc_info=True)\n",
    "        raise Exception(\"Could not insert data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.close()\n",
    "con.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
